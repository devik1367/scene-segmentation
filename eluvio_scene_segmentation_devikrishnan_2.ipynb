{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "eluvio-scene-segmentation-devikrishnan-2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "R89xhLxzlhmn"
      },
      "source": [
        "# uncomment the below line of code to install dependencies the first time the script is run\n",
        "# !pip install numpy scipy pandas scikit-learn"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHalnXAaZAny"
      },
      "source": [
        "# imports\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import glob\n",
        "import numpy as np\n",
        "from sklearn import *\n",
        "import tensorflow as tf"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0SW8ScunBIq",
        "outputId": "f03ac194-3e95-4e04-afd1-38adfc292eb8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKYiVuLNZCQz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c6d3c7b-c128-4345-dfb2-5d3b9173a03b"
      },
      "source": [
        "# read data\n",
        "data_path = \"/content/drive/My Drive/Colab_Notebooks/Eluvio/data/\"     # to reproduce, change the data path \n",
        "model_path = \"/content/drive/My Drive/Colab_Notebooks/Eluvio/\"         # to save model to file later\n",
        "filenames = glob.glob(data_path+\"*.pkl\")\n",
        "unpickled_data = []\n",
        "\n",
        "for f in filenames:\n",
        "  unpickled_data.append(pickle.load(open(f, \"rb\")))\n",
        "\n",
        "# look at data\n",
        "print(type(unpickled_data[0]))\n",
        "print(\"imdb_id: {}\\n\".format(unpickled_data[0]['imdb_id']))\n",
        "print(\"place: {}\\n\".format(unpickled_data[0]['place']))\n",
        "print(\"cast: {}\\n\".format(unpickled_data[0]['cast']))\n",
        "print(\"action: {}\\n\".format(unpickled_data[0]['action']))\n",
        "print(\"audio: {}\\n\".format(unpickled_data[0]['audio']))\n",
        "print(\"scene_transition_boundary_ground_truth: {}\\n\".format(unpickled_data[0]['scene_transition_boundary_ground_truth']))\n",
        "print(\"scene_transition_boundary_prediction: {}\\n\\n\".format(unpickled_data[0]['scene_transition_boundary_prediction']))\n",
        "print(\"scene_transition_boundary_ground_truth.shape: {}\\n\".format(unpickled_data[0]['scene_transition_boundary_ground_truth'].shape))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'dict'>\n",
            "imdb_id: tt0052357\n",
            "\n",
            "place: tensor([[6.1556e-02, 1.0177e-01, 5.2085e-02,  ..., 6.7148e-02, 1.5965e-03,\n",
            "         9.5967e-03],\n",
            "        [5.2110e-02, 1.3152e-01, 4.7641e-02,  ..., 7.6565e-02, 1.4197e-03,\n",
            "         1.1087e-02],\n",
            "        [8.7748e-02, 2.4885e+00, 5.4332e-02,  ..., 7.1745e-01, 8.1299e-01,\n",
            "         3.3096e-01],\n",
            "        ...,\n",
            "        [1.2349e-01, 4.4957e-01, 2.4556e-01,  ..., 1.0015e-02, 1.1907e-01,\n",
            "         5.1556e-03],\n",
            "        [1.9621e-02, 7.2868e-01, 2.4324e-01,  ..., 7.4836e-03, 3.7045e-02,\n",
            "         7.4661e-02],\n",
            "        [1.2028e-01, 2.9355e-01, 7.4574e-02,  ..., 4.0226e-02, 7.1149e-02,\n",
            "         1.5697e-01]])\n",
            "\n",
            "cast: tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ..., -0.0073,  0.1230, -0.0396],\n",
            "        ...,\n",
            "        [ 0.0000,  0.0000,  0.0000,  ..., -0.0309,  0.1172, -0.0647],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ..., -0.0290,  0.0804,  0.0003],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n",
            "\n",
            "action: tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 5.5101, 0.0000,  ..., 0.0000, 0.0132, 3.0242],\n",
            "        ...,\n",
            "        [0.0000, 5.7000, 0.0000,  ..., 0.0000, 0.0000, 0.1954],\n",
            "        [0.0000, 4.5412, 0.0000,  ..., 0.0000, 0.1389, 1.3127],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])\n",
            "\n",
            "audio: tensor([[-0.0993, -0.0711, -0.0708,  ..., -0.0829, -0.0911, -0.1051],\n",
            "        [-0.0943, -0.0590, -0.0602,  ..., -0.0682, -0.0753, -0.0831],\n",
            "        [-0.0442, -0.0324, -0.0318,  ..., -0.0332, -0.0386, -0.0454],\n",
            "        ...,\n",
            "        [ 0.0216, -0.0060, -0.0127,  ..., -0.0135, -0.0496, -0.0513],\n",
            "        [-0.0800, -0.0120, -0.0191,  ..., -0.0272, -0.0592, -0.0654],\n",
            "        [-0.1281, -0.0604, -0.0645,  ..., -0.0776, -0.0705, -0.0774]])\n",
            "\n",
            "scene_transition_boundary_ground_truth: tensor([False, False, False,  ..., False, False,  True])\n",
            "\n",
            "scene_transition_boundary_prediction: tensor([0.0000, 0.0000, 0.4878,  ..., 0.0000, 0.0000, 0.0000],\n",
            "       dtype=torch.float16)\n",
            "\n",
            "\n",
            "scene_transition_boundary_ground_truth.shape: torch.Size([1100])\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYP-IlPQilkZ",
        "outputId": "4d70499d-c94f-489b-e573-c25c74b1678e"
      },
      "source": [
        "print(\"scene_transition_boundary_prediction.shape: {}\\n\\n\".format(unpickled_data[0]['scene_transition_boundary_prediction'].shape))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "scene_transition_boundary_prediction.shape: torch.Size([1100])\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eO8UiLrBnMOd",
        "outputId": "ee87a8d6-8a20-486b-cd9b-165a340e0868"
      },
      "source": [
        "# pre-process features/target attribute so they're in the correct format for feeding into our neural network model\n",
        "# num_data_files = len(unpickled_data)    \n",
        "\n",
        "# print(len(filenames))\n",
        "# print(len(filenames[:45]))\n",
        "# print(len(filenames[45:]))\n",
        "\n",
        "def preprocess_features(list_of_filenames):\n",
        "  j = 0\n",
        "  for i in range(len(list_of_filenames)):\n",
        "    # read file\n",
        "    inp_file = open(list_of_filenames[i], \"rb\")\n",
        "    datafile = pickle.load(inp_file)\n",
        "    inp_file.close()\n",
        "\n",
        "    ft1, ft2, ft3, ft4 = datafile['place'].data.numpy(), datafile['cast'].data.numpy(), datafile['action'].data.numpy(), datafile['audio'].data.numpy()\n",
        "    x = np.hstack((ft1, ft2, ft3, ft4))                       # features\n",
        "    y = datafile['scene_transition_boundary_ground_truth']    \n",
        "\n",
        "    # pre-process features\n",
        "    scaler = preprocessing.MinMaxScaler().fit(x)\n",
        "    x_transformed = scaler.transform(x)\n",
        "\n",
        "    x_fold = np.zeros((x.shape[0] - 1, 2*x.shape[1]))\n",
        "    for i in range(x.shape[0] - 1):\n",
        "      x_fold[i,:] = np.hstack((x_transformed[i,:], x_transformed[i+1,:]))\n",
        "\n",
        "    if j==0:\n",
        "      X = x_fold\n",
        "      Y = y\n",
        "    else:\n",
        "      X = np.concatenate((X, x_fold), axis=0)\n",
        "      Y = np.concatenate((Y, y), axis=0)\n",
        "    j += 1\n",
        "  return X, Y\n",
        "\n",
        "\n",
        "# split data into train and test sets - I'm using 70/30 for train/test; 70% of 64 is 44.8 so I rounded up to 45\n",
        "X_train, y_train = preprocess_features(filenames[:45])\n",
        "X_test, y_test = preprocess_features(filenames[45:])\n",
        "\n",
        "print(\"X_train.shape: {}\\n\".format(X_train.shape))\n",
        "print(\"y_train.shape: {}\\n\".format(y_train.shape))\n",
        "print(\"X_test.shape: {}\\n\".format(X_test.shape))\n",
        "print(\"y_test.shape: {}\\n\".format(y_test.shape))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train.shape: (70480, 7168)\n",
            "\n",
            "y_train.shape: (70480,)\n",
            "\n",
            "X_test.shape: (35497, 7168)\n",
            "\n",
            "y_test.shape: (35497,)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "F5XieTyy7Z7v",
        "outputId": "5974cd6b-9c63-4c9a-fd05-6b2dac6cebbc"
      },
      "source": [
        " # convert target attribute to 0/1 instead of False/True since the final predictions will be in integer form\n",
        "def convert_labels(Y):\n",
        "  Y_converted = np.zeros((Y.shape[0]), dtype = np.uint8)\n",
        "  for i in range(Y.shape[0]):\n",
        "      if Y[i]==True:\n",
        "          Y_converted[i] = 1\n",
        "      else:\n",
        "          Y_converted[i] = 0\n",
        "  return Y_converted\n",
        "\n",
        "# convert both train and test labels\n",
        "y_train_converted = convert_labels(y_train)\n",
        "y_test_converted = convert_labels(y_test)\n",
        "\n",
        "# plot distribution of target variable: train\n",
        "target_df = pd.DataFrame(y_train_converted, columns=['scene_transition_boundary_ground_truth'])\n",
        "print(target_df['scene_transition_boundary_ground_truth'].value_counts().plot(kind='bar', title=\"Distribution of scene_transition_boundary_ground_truth in y_train\"))\n",
        "# plot distribution of target variable: test\n",
        "target_df = pd.DataFrame(y_test_converted, columns=['scene_transition_boundary_ground_truth'])\n",
        "print(target_df['scene_transition_boundary_ground_truth'].value_counts().plot(kind='bar', title=\"Distribution of scene_transition_boundary_ground_truth in y_test\"))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AxesSubplot(0.125,0.125;0.775x0.755)\n",
            "AxesSubplot(0.125,0.125;0.775x0.755)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7raSMNVeFt5",
        "outputId": "4c0132e6-79c8-48a2-da80-cca9847835f5"
      },
      "source": [
        "# evaluate preliminary predictions\n",
        "# to reproduce, change directory to the directory where your data directory is\n",
        "%cd /content/drive/MyDrive/Colab_Notebooks/Eluvio/   \n",
        "!python evaluate_sceneseg.py data/"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab_Notebooks/Eluvio\n",
            "# of IMDB IDs: 64\n",
            "Scores: {\n",
            "    \"AP\": 0.4418872028438688,\n",
            "    \"mAP\": 0.45644015956781614,\n",
            "    \"Miou\": 0.4541480053002175,\n",
            "    \"Precision\": 0.2761656092479825,\n",
            "    \"Recall\": 0.7473442326299846,\n",
            "    \"F1\": 0.39309552999275693\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBnmo0uPoE3K"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r80wTEEXlJIC"
      },
      "source": [
        "from numpy import loadtxt\n",
        "from keras.models import Sequential, save_model, load_model\n",
        "from keras.layers import Dense"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcyXA-BA2a9Z"
      },
      "source": [
        "# define the keras model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=X_train.shape[1], activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the keras model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit the keras model on the dataset\n",
        "model.fit(x=X_train, y=y_train_converted, epochs=50)\n",
        "# evaluate the keras model\n",
        "_, accuracy = model.evaluate(X_train, y_train_converted, verbose=0)\n",
        "print('Accuracy: %.2f' % (accuracy*100))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "id": "oTKaaZJ3lagE",
        "outputId": "204f4c2a-6a79-4823-9689-7758c1c1b7e4"
      },
      "source": [
        "# save model to file\n",
        "# save_model(model, model_path+\"seq_model_2.h5\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-25e424f0db7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# save model to file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"seq_model_2.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Fxrnnwh4THA"
      },
      "source": [
        "# load model from file\n",
        "model = load_model(model_path+\"seq_model_2.h5\")\n",
        "# make probability predictions with the model\n",
        "predictions = model.predict(X_test)\n",
        "# round predictions \n",
        "# y_pred = [round(x[0]) for x in predictions]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpKry5aTiIWP",
        "outputId": "7965161c-8d91-42b8-8d10-1283860cd581"
      },
      "source": [
        "print(\"type(predictions): {}\".format(type(predictions)))\n",
        "print(\"predictions.shape: {}\".format(predictions.shape))\n",
        "print(predictions)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "type(predictions): <class 'numpy.ndarray'>\n",
            "predictions.shape: (35497, 1)\n",
            "[[0.03319377]\n",
            " [0.02613917]\n",
            " [0.00671345]\n",
            " ...\n",
            " [0.04637426]\n",
            " [0.30535126]\n",
            " [0.40110606]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2JazOym7xEi"
      },
      "source": [
        "# save predictions to .pkl files similar to the input files so we can run the metrics\n",
        "# fields needed for evaluation are \"scene_transition_boundary_ground_truth\", \"scene_transition_boundary_prediction\", \"shot_end_frame\", \"imdb_id\"\n",
        "for i in range(len(filenames)):\n",
        "  infile = open(filenames[i], \"rb\")\n",
        "  datafile = pickle.load(infile)\n",
        "  infile.close()\n",
        "  # eval_features = [\"scene_transition_boundary_ground_truth\", \"scene_transition_boundary_prediction\", \"shot_end_frame\", \"imdb_id\"]\n",
        "  # convert ground truth and predicitons to tensor format\n",
        "  # gt = tf.convert_to_tensor(datafile[\"scene_transition_boundary_ground_truth\"].data.numpy(), np.float32)\n",
        "  # pred = tf.convert_to_tensor(y_pred, np.float32)\n",
        "  eval_dict = dict([\n",
        "      (\"scene_transition_boundary_ground_truth\", datafile[\"scene_transition_boundary_ground_truth\"].data.numpy()),\n",
        "      (\"scene_transition_boundary_prediction\", predictions),\n",
        "      (\"shot_end_frame\", datafile[\"shot_end_frame\"].data.numpy()),\n",
        "      (\"imdb_id\", datafile[\"imdb_id\"])\n",
        "  ])\n",
        "  # write eval_dict to .pkl file\n",
        "  with open(data_path+\"outs/\"+datafile[\"imdb_id\"]+\".pkl\", \"wb\") as f:\n",
        "    pickle.dump(eval_dict, f)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igq2ciEVB1TQ",
        "outputId": "3687af97-c28f-4fc5-9310-aba5165d6156"
      },
      "source": [
        "# check that the output files were created correctly\n",
        "check_file = open(data_path+\"outs/tt0112573.pkl\", \"rb\")\n",
        "check_outs = pickle.load(check_file)\n",
        "check_file.close()\n",
        "print(type(check_outs))\n",
        "print(\"imdb_id: {}\\n\".format(check_outs['imdb_id']))\n",
        "print(\"scene_transition_boundary_ground_truth: {}\\n\".format(check_outs['scene_transition_boundary_ground_truth']))\n",
        "print(\"scene_transition_boundary_prediction: {}\\n\\n\".format(check_outs['scene_transition_boundary_prediction']))\n",
        "print(\"scene_transition_boundary_ground_truth.shape: {}\\n\".format(check_outs['scene_transition_boundary_ground_truth'].shape))\n",
        "print(\"scene_transition_boundary_prediction.shape: {}\\n\".format(check_outs['scene_transition_boundary_prediction'].shape))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'dict'>\n",
            "imdb_id: tt0112573\n",
            "\n",
            "scene_transition_boundary_ground_truth: [False  True  True ... False False False]\n",
            "\n",
            "scene_transition_boundary_prediction: [[0.03319377]\n",
            " [0.02613917]\n",
            " [0.00671345]\n",
            " ...\n",
            " [0.04637426]\n",
            " [0.30535126]\n",
            " [0.40110606]]\n",
            "\n",
            "\n",
            "scene_transition_boundary_ground_truth.shape: (3095,)\n",
            "\n",
            "scene_transition_boundary_prediction.shape: (35497, 1)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXHR613nmanB"
      },
      "source": [
        "# evaluate performance of generated predictions\n",
        "%cd /content/drive/MyDrive/Colab_Notebooks/Eluvio/  \n",
        "!python evaluate_sceneseg.py data/outs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfaXs47mpaGW"
      },
      "source": [
        "20 epochs:\n",
        "Scores: {\n",
        "    \"AP\": 0.4418872028438688,\n",
        "    \"mAP\": 0.45644015956781614,\n",
        "    \"Miou\": 0.4541480053002175,\n",
        "    \"Precision\": 0.2761656092479825,\n",
        "    \"Recall\": 0.7473442326299846,\n",
        "    \"F1\": 0.39309552999275693\n",
        "}\n",
        "\n",
        "50 epochs: "
      ]
    }
  ]
}